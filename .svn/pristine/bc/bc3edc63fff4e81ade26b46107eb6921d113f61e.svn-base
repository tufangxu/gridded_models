import yaml
import psycopg2
from psycopg2.extensions import AsIs
import os.path
import subprocess
from urllib.request import urlretrieve
import glob
from osgeo import gdal
import re
import datetime


with open(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir, 'config.yml')), 'r') as ymlfile:
    cfg = yaml.load(ymlfile)
db = cfg["postgis"]
conn = psycopg2.connect(dbname=db["db"], port=db["port"], user=db["user"],
                        password=db["password"], host=db["host"])
curs = conn.cursor()

save_path = cfg["ndfd_path"]

# download next 7 days of min and max temp predictions starting with tomorrow
# two files are provided with 3 bands in the first and 4 bands in the second
# each band represents a day
url_tmax1 = 'http://weather.noaa.gov/pub/SL.us008001/ST.opnl/DF.gr2/DC.ndfd/AR.conus/VP.001-003/ds.maxt.bin'
url_tmax2 = 'http://weather.noaa.gov/pub/SL.us008001/ST.opnl/DF.gr2/DC.ndfd/AR.conus/VP.004-007/ds.maxt.bin'
url_tmin1 = 'http://weather.noaa.gov/pub/SL.us008001/ST.opnl/DF.gr2/DC.ndfd/AR.conus/VP.001-003/ds.mint.bin'
url_tmin2 = 'http://weather.noaa.gov/pub/SL.us008001/ST.opnl/DF.gr2/DC.ndfd/AR.conus/VP.004-007/ds.mint.bin'

forecasts = ({'url': url_tmax1, 'file_name': 'maxt1-3.bin'},
             {'url': url_tmax2, 'file_name': 'maxt4-7.bin'},
             {'url': url_tmin1, 'file_name': 'mint1-3.bin'},
             {'url': url_tmin2, 'file_name': 'mint4-7.bin'})


def download_forecast():
    os.makedirs(save_path, exist_ok=True)

    # delete any files from previous last imports
    for bin_file in glob.glob(save_path + "*.bin"):
        os.remove(bin_file)
    for bil_file in glob.glob(save_path + "*.bil"):
        os.remove(bil_file)
    for xml_file in glob.glob(save_path + "*.xml"):
        os.remove(xml_file)
    for hdr_file in glob.glob(save_path + "*.hdr"):
        os.remove(hdr_file)

    for forecast in forecasts:
        print('retrieving ' + save_path + forecast['file_name'])
        urlretrieve(forecast['url'], save_path + forecast['file_name'])


def extract_bands():
    for bin_file in glob.glob(save_path + "*.bin"):
        forecast_data = gdal.Open(bin_file)
        num_bands = forecast_data.RasterCount
        for band in range(1, num_bands+1):
            data_band = forecast_data.GetRasterBand(band)
            meta = data_band.GetMetadata()
            element = meta['GRIB_ELEMENT']
            timestamp = re.findall('\d+', meta['GRIB_VALID_TIME'])[0]


            dest_file = save_path + element + '_' + timestamp + '.bin'
            extract_command = "gdal_translate -b {band_number} {source_file} {dest_file}"\
                .format(band_number=band, source_file=bin_file, dest_file=dest_file)
            ps = subprocess.Popen(extract_command, stdout=subprocess.PIPE, shell=True)
            ps.wait()
        forecast_data = None
        os.remove(bin_file)


def warp_to_prism():
    # reproject, crop, and rescale so that ndfd raster overlays prism raster nicely
    for bin_file in glob.glob(save_path + "*.bin"):
        warped_file = str.replace(bin_file, ".bin", ".bil")
        warp_command = "gdalwarp -of ENVI -srcnodata 9999 -dstnodata -9999 -t_srs EPSG:4269 -te -125.0208333 24.0625000 -66.4791667 49.9375000 -ts 1405 621 {source_file} {dest_file}"\
            .format(source_file=bin_file, dest_file=warped_file)
        ps = subprocess.Popen(warp_command, stdout=subprocess.PIPE, shell=True)
        ps.wait()


def postgis_import():
    for ndfd_file in glob.glob(save_path + "*.bil") + glob.glob(save_path + "*.bin"):
        table_name = None
        resolution = None
        # the bil files are rescaled to 4k
        # the bin files are the original 2.5k resolution
        if ".bin" in ndfd_file:
            resolution = '2.5k'
            if "MaxT" in ndfd_file:
                table_name = "tmax_forecast_2p5k"
            elif "MinT" in ndfd_file:
                table_name = "tmin_forecast_2p5k"
        elif ".bil" in ndfd_file:
            resolution = '4k'
            if "MaxT" in ndfd_file:
                table_name = "tmax_forecast_4k"
            elif "MinT" in ndfd_file:
                table_name = "tmin_forecast_4k"

        new_table = True
        query = "SELECT COUNT(*) FROM information_schema.tables WHERE table_name = %s;"
        curs.execute(query, [table_name])
        if curs.fetchone()[0] == 1:
            new_table = False

        # remove old entry if already exists
        if not new_table:
            query = "DELETE FROM %(table)s WHERE filename = %(filename)s;"
            data = {"table": AsIs(table_name), "filename": os.path.basename(ndfd_file)}
            curs.execute(query, data)
            conn.commit()

        # insert the raster (either create a new table or append to previously created table)
        if new_table:
            import_command = "raster2pgsql -s 4269 -c -I -C -M -F -t auto {file} public.{table}"\
                .format(file=ndfd_file, table=table_name)
        else:
            import_command = "raster2pgsql -s 4269 -a -M -F -t auto {file} public.{table}"\
                .format(file=ndfd_file, table=table_name)
        import_command2 = "psql -h {host} -p {port} -d {database} -U {user} --no-password"\
            .format(host=db["host"], port=db["port"], database=db["db"], user=db["user"])
        ps = subprocess.Popen(import_command, stdout=subprocess.PIPE, shell=True)
        subprocess.check_output(import_command2, stdin=ps.stdout, shell=True)
        ps.wait()

        # possibly set up extra table structure
        if new_table:
            query = "ALTER TABLE %(table)s ADD forecast_date date;"
            curs.execute(query, {"table": AsIs(table_name)})
            conn.commit()
        forecast_unixtime = re.findall('\d+', ndfd_file)[0]
        forecast_date = datetime.datetime.fromtimestamp(int(forecast_unixtime)).strftime('%Y-%m-%d')
        query = "UPDATE %(table)s SET forecast_date = to_date(%(forecast_date)s, 'YYYY-MM-DD') WHERE forecast_date IS NULL;"
        data = {"table": AsIs(table_name), "forecast_date": forecast_date}
        curs.execute(query, data)
        conn.commit()

        # create entry in mosaic table (for geoserver to work)
        if new_table:
            query = """
              CREATE TABLE IF NOT EXISTS mosaic(
              name text,
              tiletable text,
              minx float,
              miny float,
              maxx float,
              maxy float,
              resx float,
              resy float);"""
            curs.execute(query)
            conn.commit()

            query = "DELETE FROM mosaic WHERE tiletable = %s"
            curs.execute(query, [table_name])
            conn.commit()

            query = """
              INSERT INTO mosaic (name, tiletable, minx, miny, maxx, maxy, resx, resy)
              VALUES (%s, %s, %s, %s, %s, %s, %s, %s);"""
            if resolution == '4k':
                data = (table_name, table_name, -125, 49.9166666666687, -66.9, 24.2, 0.04166666666667, 0.04166666666667)
            elif resolution == '2.5k':
                data = (table_name, table_name, -2764486.928, -265060.521, 2683176.007, 3232110.510, 2539.703000000000000, 2539.703000000000000)
            curs.execute(query, data)
            conn.commit()


if __name__ == "__main__":
    download_forecast()
    extract_bands()
    warp_to_prism()
    postgis_import()
