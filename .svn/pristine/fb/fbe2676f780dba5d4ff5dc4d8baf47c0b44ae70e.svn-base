import os.path
from urllib.request import urlretrieve
import urllib
import glob
import ftplib
from datetime import timedelta
import datetime as dt
import numpy as np
from util.database import *
from util.raster import *
from shutil import copy
from datetime import *
import re
import random


with open(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir, 'config.yml')), 'r') as ymlfile:
    cfg = yaml.load(ymlfile)


def download_forecast():
    # download next 7 days of min and max temp predictions starting with tomorrow
    # two files are provided with 3 bands in the first and 4 bands in the second
    # each band represents a day

    working_path = cfg["temp_path"]
    tmin_save_path = cfg["daily_tmin_path"]
    tmax_save_path = cfg["daily_tmax_path"]
    os.makedirs(working_path, exist_ok=True)
    os.makedirs(tmin_save_path, exist_ok=True)
    os.makedirs(tmax_save_path, exist_ok=True)
    cleanup(working_path)

    url_tmax1 = 'http://weather.noaa.gov/pub/SL.us008001/ST.opnl/DF.gr2/DC.ndfd/AR.conus/VP.001-003/ds.maxt.bin'
    url_tmax2 = 'http://weather.noaa.gov/pub/SL.us008001/ST.opnl/DF.gr2/DC.ndfd/AR.conus/VP.004-007/ds.maxt.bin'
    url_tmin1 = 'http://weather.noaa.gov/pub/SL.us008001/ST.opnl/DF.gr2/DC.ndfd/AR.conus/VP.001-003/ds.mint.bin'
    url_tmin2 = 'http://weather.noaa.gov/pub/SL.us008001/ST.opnl/DF.gr2/DC.ndfd/AR.conus/VP.004-007/ds.mint.bin'

    forecasts = ({'url': url_tmax1, 'file_name': 'maxt1-3.bin'},
                 {'url': url_tmax2, 'file_name': 'maxt4-7.bin'},
                 {'url': url_tmin1, 'file_name': 'mint1-3.bin'},
                 {'url': url_tmin2, 'file_name': 'mint4-7.bin'})

    for forecast in forecasts:
        # grab the multiband raster
        file_name = forecast['file_name']
        print('retrieving ' + working_path + file_name)
        urlretrieve(forecast['url'], working_path + file_name)

        # each band is a different day
        forecast_data = gdal.Open(working_path + file_name)
        num_bands = forecast_data.RasterCount
        for band in range(1, num_bands+1):
            data_band = forecast_data.GetRasterBand(band)
            meta = data_band.GetMetadata()
            element = meta['GRIB_ELEMENT']

            # get timestamp in utc for when the raster map is valid
            rast_utc = int(re.findall('\d+', meta['GRIB_VALID_TIME'])[0])
            rast_date = datetime.fromtimestamp(rast_utc, timezone.utc)

            file = working_path + element + '_' + rast_date.strftime("%Y%m%d") + '.bin'
            if element == 'MinT':
                dest_file = tmin_save_path + 'tmin_' + rast_date.strftime("%Y%m%d") + '.tif'
                table_name = "tmin_" + str(rast_date.year)
            elif element == 'MaxT':
                dest_file = tmax_save_path + 'tmax_' + rast_date.strftime("%Y%m%d") + '.tif'
                table_name = "tmax_" + str(rast_date.year)
            else:
                print('invalid element: ' + element)
                continue
            extract_command = "gdal_translate -b {band_number} {source_file} {dest_file}"\
                .format(band_number=band, source_file=working_path + file_name, dest_file=file)
            ps = subprocess.Popen(extract_command, stdout=subprocess.PIPE, shell=True)
            ps.wait()

            # warp to match prism extent, projection, and size
            warp_to_prism(file)

            src_ds = gdal.Open(file)
            rast_band = src_ds.GetRasterBand(1)

            # create new raster with non land areas masked out
            rast_array = rast_band.ReadAsArray()
            apply_usa_mask(rast_array)

            write_raster(dest_file, rast_array, -9999, src_ds.RasterXSize, src_ds.RasterYSize, src_ds.GetProjection(), src_ds.GetGeoTransform())

            # import raster to db
            rtma_import(dest_file, table_name, False, rast_date, rast_date.hour, 'ndfd')
            src_ds = None
        forecast_data = None
    cleanup(working_path)


def download_hourly_temps(dataset):
    working_path = cfg["temp_path"]
    save_path = cfg["hourly_temp_path"]
    os.makedirs(working_path, exist_ok=True)
    os.makedirs(save_path, exist_ok=True)
    cleanup(working_path)

    if dataset == 'rtma':
        base_url_temp = 'ftp://tgftp.nws.noaa.gov/SL.us008001/ST.opnl/DF.gr2/DC.ndgd/GT.rtma/AR.conus/'
        # base_url_temp = 'http://weather.noaa.gov/pub/SL.us008001/ST.opnl/DF.gr2/DC.ndgd/GT.rtma/AR.conus/'
    elif dataset == 'urma':
        base_url_temp = 'ftp://tgftp.nws.noaa.gov/SL.us008001/ST.opnl/DF.gr2/DC.ndgd/GT.urma/AR.conus/'
        # base_url_temp = 'http://weather.noaa.gov/pub/SL.us008001/ST.opnl/DF.gr2/DC.ndgd/GT.urma/AR.conus/'
    else:
        print('invalid dataset: ' + dataset)

    # hour is UTC
    for hour in range(0, 24):
        zero_padded_hour = "{0:0=2d}".format(hour)

        # download the file (we keep retrying if there are network issues)
        url = base_url_temp + 'RT.' + zero_padded_hour + '/ds.temp.bin'
        print('retrieving ' + url)
        retrieved = False
        file_name = dataset + '_' + zero_padded_hour + '.bin'
        while not retrieved:
            try:
                urlretrieve(url, working_path + file_name)
            except urllib.error.URLError as e:
                print('error retrieving file (retrying): ' + str(e))
            except urllib.error.ContentTooShortError as e:
                print('error retrieving file (retrying): ' + str(e))
            else:
                retrieved = True

        # warp to match prism extent, projection, and size
        warp_to_prism(working_path + file_name)

        src_ds = gdal.Open(working_path + file_name)
        rast_band = src_ds.GetRasterBand(1)

        # get timestamp in utc for when the raster map is valid
        band_meta_data = rast_band.GetMetadata()
        rast_utc = int(re.findall('\d+', band_meta_data["GRIB_VALID_TIME"])[0])
        rast_date = datetime.fromtimestamp(rast_utc, timezone.utc)
        table_name = "hourly_temp_" + str(rast_date.year)

        # create new raster with non land areas masked out
        rast_array = rast_band.ReadAsArray()
        apply_usa_mask(rast_array)
        masked_file_path = save_path + dataset + '_' + rast_date.strftime("%Y%m%d") + zero_padded_hour + '.tif'
        write_raster(masked_file_path, rast_array, -9999, src_ds.RasterXSize, src_ds.RasterYSize, src_ds.GetProjection(), src_ds.GetGeoTransform())

        # import raster to db
        rtma_import(masked_file_path, table_name, True, rast_date, rast_date.hour, dataset)
        src_ds = None
        cleanup(working_path)


def compute_tmin_tmax(start_date, end_date):
    # hourly_temp_path = cfg["hourly_temp_path"]
    tmin_save_path = cfg["daily_tmin_path"]
    tmax_save_path = cfg["daily_tmax_path"]
    os.makedirs(tmin_save_path, exist_ok=True)
    os.makedirs(tmax_save_path, exist_ok=True)

    delta = end_date - start_date
    retrieved_raster_info = False
    for i in range(delta.days + 1):
        day = start_date + timedelta(days=i)
        tmin_file_name = 'tmin_' + day.strftime("%Y%m%d") + '.tif'
        tmax_file_name = 'tmax_' + day.strftime("%Y%m%d") + '.tif'

        # skip if the file already has been imported and is older than 3 days
        today = dt.datetime.now().date()
        day_dif = (today - day).days
        if day_dif > 3 and os.path.isfile(tmin_save_path + tmin_file_name) and os.path.isfile(tmax_save_path + tmax_file_name):
            continue

        temps = []
        hourly_temp_table_name = 'hourly_temp' + "_" + day.strftime("%Y")
        daily_tmin_table_name = "tmin_" + str(day.year)
        daily_tmax_table_name = "tmax_" + str(day.year)
        contains_urma = False
        contains_rtma = False

        for hour in range(0,24):
            arr = get_climate_data(hourly_temp_table_name, day, hour, 'urma')
            if arr is not None:
                temps.append(arr)
                if not retrieved_raster_info:
                    (rast_cols, rast_rows, transform, projection, no_data_value) = get_raster_info(hourly_temp_table_name, day, hour, 'urma')
                    retrieved_raster_info = True
                contains_urma = True
            else:
                arr = get_climate_data(hourly_temp_table_name, day, hour, 'rtma')
                if arr is not None:
                    temps.append(arr)
                    if not retrieved_raster_info:
                        (rast_cols, rast_rows, transform, projection, no_data_value) = get_raster_info(hourly_temp_table_name, day, hour, 'rtma')
                        retrieved_raster_info = True
                    contains_rtma = True

        if len(temps) is 24:
            tmin = np.minimum.reduce(temps)
            tmax = np.maximum.reduce(temps)
            tmin[tmin == np.nan] = -9999
            tmax[tmax == np.nan] = -9999

            tmin_path = tmin_save_path + tmin_file_name
            tmax_path = tmax_save_path + tmax_file_name
            write_raster(tmin_path, tmin, -9999, tmin.shape[1], tmin.shape[0], projection, transform)
            write_raster(tmax_path, tmax, -9999, tmax.shape[1], tmax.shape[0], projection, transform)

            # save tmin/tmax rasters to the db
            if contains_rtma and contains_urma:
                source_data_string = "urma_and_rtma"
            elif contains_urma:
                source_data_string = "urma"
            else:
                source_data_string = "rtma"
            rtma_import(tmin_path, daily_tmin_table_name, False, day, None, source_data_string)
            rtma_import(tmax_path, daily_tmax_table_name, False, day, None, source_data_string)

        else:
            print('not enough hours to compute tmin/tmax for: ' + day.strftime("%Y%m%d"))
            print('the day only had ' + str(len(temps)) + ' hourly temps recorded')


def download_historical_temps(start_date, end_date):
    working_path = cfg["temp_path"]
    save_path = cfg["hourly_temp_path"]
    tmin_save_path = cfg["daily_tmin_path"]
    tmax_save_path = cfg["daily_tmin_path"]
    os.makedirs(working_path, exist_ok=True)
    os.makedirs(save_path, exist_ok=True)
    os.makedirs(tmin_save_path, exist_ok=True)
    os.makedirs(tmax_save_path, exist_ok=True)
    cleanup(working_path)

    base_url_temp = 'http://nomads.ncdc.noaa.gov/data/ndgd/'

    delta = end_date - start_date
    year = None
    previous_file_name = None

    for i in range(delta.days + 1):
        day = start_date + timedelta(days=i)

        # rtma data is only historical, never look for today or in the future
        if day >= dt.datetime.today().date():
            continue

        # hit a new year
        if year != day.year:
            year = day.year
            hourly_table_name = 'hourly_temp_' + str(year)

        for hour in range(0, 24):
            url_file_name = 'LTIA98_KWBR_' + day.strftime("%Y%m%d") + "{0:0=2d}".format(hour) + "00"
            url = base_url_temp + day.strftime("%Y%m") + '/' + day.strftime("%Y%m%d") + '/' \
                  + url_file_name
            file_name = 'rtma_' + day.strftime("%Y%m%d") + "{0:0=2d}".format(hour)

            # skip if the file already has been imported
            if os.path.isfile(save_path + file_name + '.tif'):
                previous_file_name = file_name
                continue

            print('retrieving ' + url)
            retrieved = False
            file_not_found = False
            while not retrieved and not file_not_found:
                try:
                    urlretrieve(url, working_path + file_name)
                except urllib.error.URLError as e:
                    if str(e) == "HTTP Error 404: Not Found":
                        print("ERROR: file not found so copying last retrieved hour to this hour: " + str(e))
                        file_not_found = True
                    else:
                        print("ERROR: counldn't retrieve file (retrying): " + str(e))
                except urllib.error.ContentTooShortError as e:
                    print("ERROR: counldn't retrieve file (retrying): " + str(e))
                else:
                    retrieved = True

            if retrieved:
                # warp the downloaded raster to EPSG 4269
                warp_to_prism(working_path + file_name)
                ds = gdal.Open(working_path + file_name)

                # hack to grab the temp band rather than the temp error band
                # by checking if the mean temp makes any sense (between -30 and 50 celsius)
                # todo find a better way
                temp_band_found = False
                band1 = ds.GetRasterBand(1)
                band2 = ds.GetRasterBand(2)
                if band1 is not None:
                    mean = band1.GetStatistics(0,1)[2]
                    if -30 < mean < 50:
                        temps_array = band1.ReadAsArray()
                        temp_band_found = True
                if band2 is not None:
                    mean = band2.GetStatistics(0,1)[2]
                    if -30 < mean < 50:
                        temps_array = band2.ReadAsArray()
                        temp_band_found = True
                if not temp_band_found:
                    print('ERROR: temperature band not found, using previous hour')

                if temp_band_found:
                    projection = ds.GetProjection()
                    transform = ds.GetGeoTransform()

                    # mask out non land areas
                    apply_usa_mask(temps_array)

                    # write masked file to disk
                    write_raster(save_path + file_name + '.tif', temps_array, -9999, temps_array.shape[1], temps_array.shape[0], projection, transform)

                    # import raster into database
                    rtma_import(save_path + file_name + '.tif', hourly_table_name, True, day, hour, "rtma")

            if not retrieved or not temp_band_found:
                if previous_file_name == None:
                    print("ERROR: The first file you tried to retrieve either doesn't exist or doesn't have a temp band. Try rerunning this script starting with an earlier date.")
                    return
                # copy last successfully retrieved temp band in place of the missing hour
                copy(save_path + previous_file_name + '.tif', save_path + file_name + '.tif')
                rtma_import(save_path + file_name + '.tif', hourly_table_name, True, day, hour, "rtma")

            previous_file_name = file_name
            ds = None
            cleanup(working_path)


def rtma_import(rast_path, table_name, hourly, date, hour, dataset):
    curs = conn.cursor()
    new_table = not table_exists(table_name)

    save_raster_to_postgis(rast_path, table_name, None)

    if new_table:
        # create entry in mosaic table (for geoserver to work)
        # add_mosaic_entry(table_name, -2764486.928, -265060.521, 2683176.007, 3232110.510, 2539.703000000000000, 2539.703000000000000)
        #add_mosaic_entry(table_name, -130.1228935, 52.8168964, -60.8601260, 20.1788770, 0.026578191679851, 0.026578191679851)

        # add rast_date and possible rast_hour columns
        query = "ALTER TABLE %(table)s ADD rast_date DATE;"
        curs.execute(query, {"table": AsIs(table_name)})
        query = "ALTER TABLE %(table)s ADD dataset TEXT;"
        curs.execute(query, {"table": AsIs(table_name)})
        query = "CREATE INDEX ON %(table)s (rast_date);"
        curs.execute(query, {"table": AsIs(table_name)})
        query = "CREATE INDEX ON %(table)s (filename);"
        curs.execute(query, {"table": AsIs(table_name)})
        if hourly:
            query = "ALTER TABLE %(table)s ADD rast_hour INTEGER;"
            curs.execute(query, {"table": AsIs(table_name)})
        conn.commit()

    query = "UPDATE %(table)s SET rast_date = to_date(%(rast_date)s, 'YYYYMMDD') WHERE rast_date IS NULL;"
    data = {"table": AsIs(table_name), "rast_date": date.strftime("%Y%m%d")}
    curs.execute(query, data)

    if hourly:
        query = "UPDATE %(table)s SET rast_hour = %(rast_hour)s WHERE rast_hour IS NULL;"
        data = {"table": AsIs(table_name), "rast_hour": "{0:0=2d}".format(hour)}
        curs.execute(query, data)

    query = "UPDATE %(table)s SET dataset = %(dataset)s WHERE dataset IS NULL;"
    data = {"table": AsIs(table_name), "dataset": dataset}
    curs.execute(query, data)

    conn.commit()


def get_raster_info(table_name, rast_date, hour, data_set):
    try:
        vsipath = '/vsimem/from_postgis'

        curs = conn.cursor()

        gdal.Unlink(vsipath)
        query = "SELECT ST_AsGDALRaster(ST_Union(rast), 'Gtiff') FROM %s WHERE rast_date = %s AND rast_hour = %s AND dataset = %s;"
        data = (AsIs(table_name), rast_date.strftime("%Y%m%d"), hour, data_set)
        curs.execute(query, data)

        result = curs.fetchone()
        if result[0] == 0:
            curs.close()
            return None

        gdal.FileFromMemBuffer(vsipath, bytes(result[0]))

        curs.close()

        # Read first band of raster with GDAL
        ds = gdal.Open(vsipath)
        band = ds.GetRasterBand(1)

        # Grab all the info to return
        num_cols = ds.RasterXSize
        num_rows = ds.RasterYSize
        transform = ds.GetGeoTransform()
        projection = ds.GetProjection()
        no_data_value = band.GetNoDataValue()

        # Close and clean up virtual memory file
        gdal.Unlink(vsipath)

        return (num_cols, num_rows, transform, projection, no_data_value)
    except Exception as e:
        gdal.Unlink(vsipath)
        return None


def get_climate_data(table_name, date, hour, data_set):
    try:
        # Load raster from postgis into a virtual memory file
        vsipath = '/vsimem/from_postgis'

        curs = conn.cursor()

        query = "SELECT ST_AsGDALRaster(ST_Union(rast), 'Gtiff') FROM %s WHERE rast_date = %s AND rast_hour = %s AND dataset = %s;"
        data = (AsIs(table_name), date.strftime("%Y%m%d"), hour, data_set)
        curs.execute(query, data)

        result = curs.fetchone()
        if result[0] is None:
            curs.close()
            return None

        gdal.FileFromMemBuffer(vsipath, bytes(result[0]))

        curs.close()

        # Read first band of raster with GDAL
        ds = gdal.Open(vsipath)
        band = ds.GetRasterBand(1)

        outarray = band.ReadAsArray()

        # Close and clean up virtual memory file
        gdal.Unlink(vsipath)

        # convert -9999 values to not a number so we don't have to worry about manipulating them
        outarray[outarray == -9999.0] = np.nan

        return outarray
    except Exception as e:
        return None


def warp_to_prism(bin_file):
    if '.bin' in bin_file:
        temp_file = str.replace(bin_file, ".bin", "_warpme.bin")
    else:
        temp_file = bin_file + "_warpme.bin"
    os.rename(bin_file, temp_file)

    extent = "-125.0208333 24.0625000 -66.4791667 49.9375000"
    warp_command = "gdalwarp -of ENVI -srcnodata 9999 -dstnodata -9999 -t_srs EPSG:4269 -ts 2606 1228 -te {extent} {source_file} {dest_file}"\
        .format(extent=extent, source_file=temp_file, dest_file=bin_file)
    ps = subprocess.Popen(warp_command, stdout=subprocess.PIPE, shell=True)
    ps.wait()
    os.remove(temp_file)


def cleanup(path):
    for file in glob.glob(path + "*"):
        os.remove(file)


if __name__ == "__main__":
    download_hourly_temps()
